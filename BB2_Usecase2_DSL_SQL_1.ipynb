{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8d697d-355c-45e7-8fe8-57e131b666c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import *\n",
    "##schema\n",
    "#schema = StructType([StructField(\"shipment_id\", IntegerType(), True),StructField(\"order_id\", StringType(), True),StructField(\"source_city\", StringType(), True),StructField(\"destination_city\", StringType(), True),StructField(\"shipment_status\", StringType(), True),StructField(\"cargo_type\", StringType(), True),StructField(\"vehicle_type\", StringType(), True),StructField(\"shipment_weight_kg\", DoubleType(), True),StructField(\"shipment_cost\", DoubleType(), True),StructField(\"shipment_date\", DateType(), True)])\n",
    "\n",
    "##dataframe creation\n",
    "dfjson = (spark.read.option(\"mode\", \"PERMISSIVE\").option(\"multiLine\", \"True\").json(\"/Volumes/workspace/default/logistics_project_data/logistics_shipment_detail_3000.json\", dateFormat=\"yyyy-MM-dd\"))\n",
    "dfjson.printSchema()\n",
    "display(dfjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a80a3e-3260-4004-9239-d9738bdccdb4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768030814963}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##logistics_source1 - read csv\n",
    "dfls1=spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header=True,inferSchema=True)\n",
    "display(dfls1)\n",
    "##logistics_source2 -read csv\n",
    "dfls2=spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source2\",header=True,inferSchema=True)\n",
    "display(dfls2)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88238cc-cf46-42e5-8b35-c6119c0946ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- Header, No comments, NO footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "####- Data Quality \n",
    "- Null columns & null rows are there\n",
    "- duplicate rows & Duplicate keys\n",
    "- format issues are there (age & shipment_id  is not in number format eg.ten)\n",
    "- No Uniformity issues found manually (Artist, artist)\n",
    "- Number of columns are more or less than the expected\n",
    "- eg. 5000006,John,Mathews,ten,Supervisor,Additionalcolumn\n",
    "- 5000004,Suresh,,52,Loader\n",
    "- Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db009cab-00a3-4f31-8f3d-7670f25c01de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True, inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "dfls1.printSchema()\n",
    "display(dfls1)\n",
    "\n",
    "##Analyse the schema, datatypes, columns etc.\n",
    "struct1=\" shipment_id integer, firstname string, lastname string, age integer, profession string, corrupt_record string\"\n",
    "dfls1 = spark.read.schema(struct1).csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True ,  columnNameOfCorruptRecord='corrupt_record')\n",
    "display(dfls1)\n",
    "dfls1.printSchema()\n",
    "##Analyse the duplicate records count and summary of the dataframe.\n",
    "dfls1dup=dfls1.where(dfls1.shipment_id.isNull())\n",
    "display(dfls1dup)\n",
    "display(len(dfls1dup.collect()))\n",
    "print(len(dfls1dup.collect()))\n",
    "\n",
    "\n",
    "total_rows = dfls1.count()\n",
    "total_columns = len(dfls1.columns)\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Total Columns: {total_columns}\")\n",
    "\n",
    "distinct_rows = dfls1.select(\"shipment_id\").distinct().count()\n",
    "duplicate_rows = total_rows - distinct_rows\n",
    "\n",
    "print(f\"Distinct Rows: {distinct_rows}\")\n",
    "print(f\"Duplicate Rows: {duplicate_rows}\")\n",
    "\n",
    "display(dfls1.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##shipment_id is non-numeric OR ## age is not an integer\n",
    "#dfls1nnm=dfls1.where(\"id not rlike'[^0-9+$]'\")##where(\"id not rlike'[0-9a-zA-Z]'\")##where(dfls1.id.rlike(r'^[a-zA-Z]+$')\n",
    "\n",
    "dfls11 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record')\n",
    "display(dfls11)\n",
    "dfls11nnmag=dfls11.where(\"shipment_id rlike '[^0-9]' OR age rlike '[^0-9]'\")\n",
    "display(dfls11nnmag)\n",
    "##fewer columns than expected\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record')\n",
    "display(dfls1)\n",
    "dfls1.where(\"shipment_id is null or age is null or first_name is null or last_name is null or role is null\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7d0fb2-ae3d-4da9-99c1-f408f0faeee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, regexp_replace,col,split,size,col\n",
    "##4. more columns than expected\n",
    "struct1=\" shipment_id integer, first_name string, last_name string, age integer, role string, corrupt_record string\"\n",
    "dfls1 = spark.read.schema(struct1).csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=True , columnNameOfCorruptRecord='corrupt_record')\n",
    "display(dfls1)\n",
    "dfls1.where(\"corrupt_record is not null \").show(truncate=False)##truncate=False - show full content of data\n",
    "\n",
    "##method1\n",
    "expected_commas = 4\n",
    "extra_columns_df = dfls1.filter((col(\"corrupt_record\").isNotNull()) &(length(col(\"corrupt_record\")) -length(regexp_replace(col(\"corrupt_record\"), \",\", \"\"))> expected_commas))\n",
    "extra_columns_df.show(truncate=False)\n",
    "\n",
    "##method2\n",
    "bad_records = dfls1.where(size(split(\"corrupt_record\", \",\")) > 5)\n",
    "bad_records.show(truncate=False)\n",
    "##method3\n",
    "bad_records = dfls1.filter(size(split(\"corrupt_record\", \",\")) > 5)\n",
    "bad_records.show(truncate=False)\n",
    "##fewer columns than expected but particular column\n",
    "bad_records = dfls1.filter(size(split(\"corrupt_record\", \",\")) < 5)\n",
    "bad_records.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af275dd7-df4e-48dd-9581-1139ad306d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Read both files without enforcing schema\n",
    "##struct1=\" shipment_id integer, first_name string, last_name string, age integer, role string,corrupt_record string\"\n",
    "from pyspark.sql.functions import *\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system1\"))\n",
    "display(dfls1)\n",
    "##struct2= \"shipment_id integer, first_name string, last_name string, age integer, role string, Hub_location string, vehicle_type string,corrupt_record string\"\n",
    "dfls2=spark.read.csv(path=[\"/Volumes/workspace/default/logistics_project_data/logistics_source2\"],header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system2\"))\n",
    "display(dfls2)\n",
    "\n",
    "dfunion=dfls1.unionByName(dfls2,allowMissingColumns=True)\n",
    "display(dfunion)\n",
    "dfunion.printSchema()\n",
    "dfunion.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d5578cd3-6ebe-4cb8-91a6-3262dea286d6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768100884729}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Cleansing (removal of unwanted datasets)\n",
    "\n",
    "##Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "dfdrop=dfunion.na.drop(how = \"any\",subset=[\"shipment_id\",\"role\"])\n",
    "display(dfdrop)\n",
    "##Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "dfdrop=dfdrop.na.drop(how = \"all\",subset=[\"first_name\",\"last_name\"])\n",
    "display(dfdrop)\n",
    "##Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "dfdrop1=dfdrop.na.drop(how = \"any\",subset=[\"shipment_id\"])\n",
    "display(dfdrop1)\n",
    "\n",
    "##Scrubbing (convert raw to tidy)\n",
    "##Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "dfage=dfunion.na.fill(\"-1\",subset=['age'])\n",
    "display(dfage)\n",
    "##Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "dfvehicle=dfunion.na.fill(\"unknown\",subset=['vehicle_type'])\n",
    "display(dfvehicle)\n",
    "##Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "dfage1rplc={'ten': '-1'}\n",
    "dfage1=dfage.na.replace(dfage1rplc,subset=['age'])\n",
    "display(dfage1)\n",
    "##Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "##method1\n",
    "dfvhclerplce=dfunion.na.replace(\"Truck\",\"LMV\",subset=['vehicle_type']).na.replace(\"Bike\",\"TwoWheeler\",subset=['vehicle_type'])\n",
    "display(dfvhclerplce)\n",
    "##method2\n",
    "dict1={\"Truck\":\"LMV\",\"Bike\":\"TwoWheeler\"}\n",
    "dfvhclerplce=dfunion.na.replace(dict1,subset=['vehicle_type'])\n",
    "display(dfvhclerplce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Detail Dataframe creation <br>\n",
    "1. Read Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db604551-c69b-4749-8c13-d3b0f2cc4bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import *\n",
    "##schema\n",
    "#schema = StructType([StructField(\"shipment_id\", IntegerType(), True),StructField(\"order_id\", StringType(), True),StructField(\"source_city\", StringType(), True),StructField(\"destination_city\", StringType(), True),StructField(\"shipment_status\", StringType(), True),StructField(\"cargo_type\", StringType(), True),StructField(\"vehicle_type\", StringType(), True),StructField(\"shipment_weight_kg\", DoubleType(), True),StructField(\"shipment_cost\", DoubleType(), True),StructField(\"shipment_date\", DateType(), True)])\n",
    "\n",
    "##dataframe creation\n",
    "dfjson = (spark.read.option(\"mode\", \"PERMISSIVE\").option(\"multiLine\", \"True\").json(\"/Volumes/workspace/default/logistics_project_data/logistics_shipment_detail_3000.json\", dateFormat=\"yyyy-MM-dd\"))\n",
    "dfjson.printSchema()\n",
    "display(dfjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: logistics_shipment_detail_3000.json<br>: domain as 'Logistics'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: logistics_source1 & logistics_source2<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: (and the merged master files)\n",
    "hub_location - Convert values to initcap case<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: logistics_shipment_detail_3000.json\n",
    "Convert shipment_ref to string<br>\n",
    "Pad to 10 characters with leading zeros<br>\n",
    "Convert dispatch_date to yyyy-MM-dd<br>\n",
    "Ensure delivery_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: logistics_source1 & logistics_source2 <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: logistics_source1 & logistics_source2<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: All 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5f3ec35e-3ef6-43a4-853e-ba99e32b4c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Add a column Source File: logistics_shipment_detail_3000.json:domain as 'Logistics'\n",
    "from pyspark.sql.functions import *\n",
    "dfjson=dfjson.withColumn(\"domain\",lit(\"Logistics\"))\n",
    "display(dfjson)\n",
    "##union logistics_source1 & logistics_source2\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system1\"))\n",
    "display(dfls1)\n",
    "##struct2= \"shipment_id integer, first_name string, last_name string, age integer, role string, Hub_location string, vehicle_type string,corrupt_record string\"\n",
    "dfls2=spark.read.csv(path=[\"/Volumes/workspace/default/logistics_project_data/logistics_source2\"],header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system2\"))\n",
    "display(dfls2)\n",
    "\n",
    "dfunion=dfls1.unionByName(dfls2,allowMissingColumns=True)\n",
    "display(dfunion)\n",
    "##Column Uniformity: role - Convert to lowercase Source File: logistics_source1 & logistics_source2\n",
    "dfuniform=dfunion.withColumn(\"role\",lower(col(\"role\")))\n",
    "display(dfuniform)\n",
    "\n",
    "##vehicle_type - Convert values to UPPERCASE Source Files: logistics_shipment_detail_3000.json\n",
    "\n",
    "dfjsonupper=dfjson.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "display(dfjsonupper)\n",
    "##(and the merged master files)hub_location - Convert values to initcap case\n",
    "dfhub=dfuniform.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "display(dfhub)\n",
    "##Format Standardization Source Files: logistics_shipment_detail_3000.json Convert shipment_ref to string Pad to 10 characters with leading zeros\n",
    "##Convert shipment_ref to string\n",
    "dfshipment_string=dfjson.withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    "display(dfshipment_string)\n",
    "##Pad to 10 characters with leading zeros##\n",
    "dfshipment_string1=dfshipment_string.withColumn(\"shipment_id\", lpad(col(\"shipment_id\"), 10, \"0\"))\n",
    "display(dfshipment_string1)\n",
    "##Convert dispatch_date to yyyy-MM-dd\n",
    "\n",
    "#dfjsdt=dfjson.withColumn(\"shipment_date\",date_format(col(\"shipment_date\"),\"yyyy-MM-dd\"))-- its notconverting to a date format\n",
    "##Convert dispatch_date to yyyy-MM-dd\n",
    "dfjsdt=dfjson.withColumn(\"shipment_date\", to_date(col(\"shipment_date\"), \"yy-MM-dd\"))\n",
    "display(dfjsdt)\n",
    "dfjsdt.printSchema()\n",
    "##Ensure delivery_cost has 2 decimal precision\n",
    "dfdeci = dfjsdt.withColumn(\"shipment_cost\",col(\"shipment_cost\").cast(\"decimal(12,2)\"))\n",
    "display(dfdeci)\n",
    "dfdeci.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e4b624ba-3bc7-498b-9b72-d248a1645206",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768139601582}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##union logistics_source1 & logistics_source2\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system1\"))\n",
    "\n",
    "##struct2= \"shipment_id integer, first_name string, last_name string, age integer, role string, Hub_location string, vehicle_type string,corrupt_record string\"\n",
    "dfls2=spark.read.csv(path=[\"/Volumes/workspace/default/logistics_project_data/logistics_source2\"],header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record').withColumn(\"data_source\",lit(\"system2\"))\n",
    "\n",
    "\n",
    "dfunion=dfls1.unionByName(dfls2,allowMissingColumns=True)\n",
    "display(dfunion)\n",
    "\n",
    "#Data Type Standardization\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File: logistics_source1 & logistics_source2\n",
    "#age: Cast String to Integer\n",
    "dfuncast=dfunion.withColumn(\"age\", col(\"age\").try_cast(\"int\"))\n",
    "#cast - will fail if we have NULL values, try_cast - If conversion fails â†’ returns NULL, Does NOT fail the job\n",
    "display(dfuncast)\n",
    "\n",
    "#Source File: logistics_shipment_detail_3000.json - shipment_weight_kg: Cast to Double\n",
    "dfdouble = dfdeci.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "display(dfdouble)\n",
    "dfdouble.printSchema()\n",
    "\n",
    "#Source File: logistics_shipment_detail_3000.json<br> - is_expedited: Cast to Boolean - we have used own condition for reference\n",
    "dfbool = dfdouble.withColumn(\"is_expedited\",when(col(\"shipment_date\") < lit(\"2024-04-30\"),lit(\"True\")).otherwise(lit(\"False\")))\n",
    "display(dfbool)\n",
    "dfbool.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b03037-fb85-42e6-8501-10acce0546c5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768142111153}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Naming Standardization <br>\n",
    "# Source File: logistics_source1 & logistics_source2<br>\n",
    "# Rename: first_name to staff_first_name<br>\n",
    "# Method 1 - too costly to use\n",
    "df_fname = dfuncast.withColumn(\"staff_first_name\", col(\"first_name\")).drop(\"first_name\")\n",
    "display(df_fname)\n",
    "# Method 2 - recommended to use\n",
    "df_fname = dfuncast.withColumnRenamed(\"first_name\", \"staff_first_name\")\n",
    "display(df_fname)\n",
    "# Rename: last_name to staff_last_name<br>\n",
    "df_lname = df_fname.withColumnRenamed(\"last_name\", \"staff_last_name\")\n",
    "display(df_lname)\n",
    "# Rename: hub_location to origin_hub_city<br>\n",
    "df_hubname = df_lname.withColumnRenamed(\"hub_location\", \"origin_hub_city\")\n",
    "display(df_hubname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe550d0e-d712-4f27-8f21-ef68f65bbeba",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768145963036}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Reordering columns logically in a better standard format:<br>\n",
    "# Source File: All 3 files<br>\n",
    "# shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "from pyspark.sql.functions import *\n",
    "df_reorder = df_hubname.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"age\").withColumn(\"ingestion_timestamp\",lit(current_timestamp()))\n",
    "display(df_reorder)\n",
    "\n",
    "df_json_reorder = dfbool.select(\"shipment_id\",\"order_id\",\"shipment_status\", \"cargo_type\", \"vehicle_type\", \"payment_mode\",\"source_city\", \"destination_city\",\"shipment_weight_kg\", \"shipment_cost\",\"shipment_date\")\n",
    "display(df_json_reorder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfcff791-b692-48e6-bd01-47b05766a739",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768148143580}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply Record Level De-Duplication\n",
    "df_dd = df_reorder.distinct()\n",
    "display(df_dd)\n",
    "\n",
    "df_ddjs = df_json_reorder.distinct()\n",
    "display(df_ddjs)\n",
    "\n",
    "# Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "df_pk = df_dd.dropDuplicates([\"shipment_id\"])\n",
    "display(df_pk)\n",
    "\n",
    "df_pkjs = df_ddjs.dropDuplicates([\"order_id\"])\n",
    "display(df_pkjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8468831-95a0-48f1-a1bf-0357eb686397",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768229023824}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add Audit Timestamp\n",
    "df_reorder = df_hubname.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"age\").withColumn(\"load_dt\",lit(current_timestamp()))\n",
    "display(df_reorder)\n",
    "\n",
    "# 2. Create Full Name\n",
    "df_fullname = df_reorder.withColumn(\"full_name\",concat_ws(\" \",col(\"staff_first_name\"),col(\"staff_last_name\")))\n",
    "display(df_fullname)\n",
    "\n",
    "# 3. Define Route Segment (route_segment) Source File: logistics_shipment_detail_3000.json\n",
    "# Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\"\n",
    "df_rsegment = df_pkjs.withColumn(\"route_segment\",concat_ws(\"-\",col(\"source_city\"),col(\"destination_city\")))\n",
    "display(df_rsegment)\n",
    "\n",
    "# 4. Generate Vehicle Identifier - \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n",
    "df_vehicleid = df_rsegment.withColumn(\"vehicle_id\",concat_ws(\"_\",col(\"vehicle_type\"),col(\"shipment_id\")))\n",
    "display(df_vehicleid)\n",
    "df_vehicleid.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c646f94-3b98-4fba-b950-2ecdf99af6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "# 1. Derive Shipment Year (shipment_year)\n",
    "# Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "# Action: Extract the year component from shipment_date.\n",
    "# Result: \"2024-04-23\" -> 2024\n",
    "# 2. Derive Shipment Month \n",
    "\n",
    "df_vehicleid = df_vehicleid.\\\n",
    "withColumn(\"shipment_year\", year(\"shipment_date\")).\\\n",
    "withColumn(\"shipment_month\", month(\"shipment_date\"))\n",
    "#year() - extract the year from given date/timestamp and retrun as integer\n",
    "df_vehicleid.printSchema()\n",
    "display(df_vehicleid)\n",
    "\n",
    "# 3. Flag Weekend Operations (is_weekend)\n",
    "# Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "# Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "\n",
    "df_weekend = df_vehicleid.withColumn(\"is_weekend\",when(dayofweek(col(\"shipment_date\")).isin(1, 7), True).otherwise(False)) \n",
    "#dayofweek - return day of the week as int, isin check if the value is in the list, detect weekend in pyspark, 1-sunday, 7-saturday respectively\n",
    "df_weekend.printSchema()\n",
    "display(df_weekend)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039bf207-b0c8-44ed-8448-6ffa86f3564b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Calculate Unit Cost (cost_per_kg) - Logic: shipment_cost / shipment_weight_kg\n",
    "df_w=df_weekend.where(col(\"shipment_weight_kg\") == 0)\n",
    "display(df_w)\n",
    "df_costperkg = df_weekend.withColumn(\"cost_per_kg\",try_divide(col(\"shipment_cost\"),col(\"shipment_weight_kg\")).cast(\"decimal(12,2)\"))\n",
    "df_costperkg.printSchema()\n",
    "display(df_costperkg)\n",
    "\n",
    "# 2. Track Shipment Age (days_since_shipment) - Logic: datediff(current_date(), shipment_date)\n",
    "df_age = df_costperkg.withColumn(\"days_since_shipment\",datediff(current_date(),col(\"shipment_date\")))\n",
    "df_age.printSchema()\n",
    "display(df_age)\n",
    "\n",
    "# 3. Compute Tax Liability (tax_amount)-Logic: shipment_cost * 0.18\n",
    "df_tax = df_age.withColumn(\"tax_amount\",(col(\"shipment_cost\")*lit(0.18)).cast(\"decimal(12,2)\"))\n",
    "df_tax.printSchema()\n",
    "display(df_tax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3a2ee3-ab52-4bea-ac0e-498e9df5ec78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Remove Redundant Name Columns - Logic: df.drop(\"first_name\", \"last_name\")\n",
    "df_fullname = df_fullname.drop(\"staff_first_name\", \"staff_last_name\")\n",
    "display(df_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c935452a-8168-451c-b0f6-be9451b35181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Splitting (Extraction) Breaking one column into multiple to isolate key information.\n",
    "\n",
    "# Split Order Code:\n",
    "# Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "# order_prefix (\"ORD\")\n",
    "# order_sequence (\"100000\")\n",
    "# Method 1\n",
    "df_split = df_tax.withColumn(\"order_prefix\",split(col(\"order_id\"), \"(?<=\\\\D)(?=\\\\d)\").getItem(0)).withColumn(\"order_sequence\",split(col(\"order_id\"), \"(?<=\\\\D)(?=\\\\d)\").getItem(1))\n",
    "\n",
    "# Method 2\n",
    "df_split = df_tax.\\\n",
    "withColumn(\"order_prefix\",regexp_extract(col(\"order_id\"), \"([A-Za-z]+)\", 1)  # all letters at start\n",
    ").withColumn(\"order_sequence\",regexp_extract(col(\"order_id\"), \"([0-9]+)\", 1))    # all numbers\n",
    "\n",
    "# Method 3\n",
    "df_split = df_tax.\\\n",
    "withColumn(\"order_prefix\",substring(col(\"order_id\"), 1, 3)).\\\n",
    "withColumn(\"order_sequence\",substring(col(\"order_id\"), 4, 6))\n",
    "display(df_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ebadf3-bd6c-4022-9789-ece2fe5b32c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split Date:\n",
    "# Action: Split shipment_date into three separate columns for partitioning:\n",
    "# ship_year (2024)\n",
    "# ship_month (4)\n",
    "# ship_day (23)\n",
    "\n",
    "df_split = df_split.withColumnsRenamed({\"shipment_year\":\"ship_year\", \"shipment_month\":\"ship_month\"}).withColumn(\"ship_day\", dayofmonth(\"shipment_date\"))\n",
    "display(df_split)\n",
    "\n",
    "# Merging (Concatenation) Combining multiple columns into a single unique identifier or description.\n",
    "# Create Route ID:\n",
    "# Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key:\n",
    "# route_lane (\"Chennai->Pune\")\n",
    "df_route = df_split.withColumn(\"route_lane\",concat_ws(\"->\", col(\"source_city\"), col(\"destination_city\")))\n",
    "df_route = df_route.drop(\"route_segment\")\n",
    "display(df_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9b8970-7bb9-4c1c-8010-4184582d2321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF1: Complex Incentive Calculation\n",
    "# Scenario: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "# Action: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "# IF Role == 'Driver' AND Age > 50:\n",
    "# Bonus = 15% of Salary (Reward for Seniority)\n",
    "# IF Role == 'Driver' AND Age < 30:\n",
    "# Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "# ELSE:\n",
    "# Bonus = 0\n",
    "# Result: A new derived column projected_bonus is generated for every row in the dataset.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def calculate_bonus(role, age):\n",
    "  # if age is None:\n",
    "  #       return 0.0\n",
    "  if role == \"Driver\":\n",
    "    if age > 50:\n",
    "      return 0.15\n",
    "    elif age >30:\n",
    "      return 0.10\n",
    "    elif age < 30:\n",
    "      return 0.05\n",
    "    elif age is None:\n",
    "      return 0.0\n",
    "  else:\n",
    "    return 0.0\n",
    "  \n",
    "bonus_udf = udf(calculate_bonus, FloatType())\n",
    "df_bonus = df_fullname.withColumn(\"projected_bonus\",bonus_udf(col(\"role\"),col(\"age\")))\n",
    "display(df_bonus)\n",
    "\n",
    "df_bonus.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd56b515-f879-4bec-a3db-b1a42233bded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF2: PII Masking (Privacy Compliance)**\n",
    "# **Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "# **Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "# **Action:** Create a UDF `mask_identity(name)`.\n",
    "# **Example:**\n",
    "# * **Input:** `\"Rajesh\"`\n",
    "# * **Output:** `\"Ra****h\"`\n",
    "# <br>\n",
    "# **Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**\n",
    "\n",
    "# from pyspark.sql.functions import col, concat, substring, lit, length\n",
    "\n",
    "df_masked = df_bonus.withColumn(\"masked_name\",concat(substring(\"full_name\", 1, 2),lit(\"****\"),substring(\"full_name\", length(\"full_name\"), 1)))\n",
    "display(df_masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d1e807-00e8-4397-868f-ac52e60a3014",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768299623920}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW customers\n",
    "using csv\n",
    "options(\n",
    "  path = \"/Volumes/workspace/default/logistics_project_data/logistics_source1\",\n",
    "  header = true,\n",
    "  inferSchema = true\n",
    ");\n",
    "SELECT\n",
    "  CONCAT(\n",
    "    SUBSTRING(first_name, 1, 2),\n",
    "    REPEAT('*', LENGTH(first_name) - 3),\n",
    "    SUBSTRING(first_name, -1)\n",
    "  ) AS masked_name\n",
    "FROM customers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"â‚¹30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" â†’ **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `priority_flag` (Descending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8ba3f9c5-7825-4f9d-acc1-50150aea0502",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768320488790}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Select (Projection) - Source Files: logistics_source1 and logistics_source2, Action: Select only first_name, role, and hub_location.\n",
    "df_fname = dfuncast.select(\"first_name\",\"role\",\"hub_location\")\n",
    "display(df_fname)\n",
    "\n",
    "# 2. Filter (Selection)-Source File: json,Filter rows where shipment_status is 'DELAYED' or 'RETURNED/CANCELLED'\n",
    "df_shipment_status = df_route.filter((col(\"shipment_status\") ==\"DELAYED\")|(col(\"shipment_status\")==\"CANCELLED\"))\n",
    "display(df_shipment_status)\n",
    "\n",
    "# Action: Filter rows where age > 50.\n",
    "df_age = df_bonus.filter(col(\"age\") > 50)\n",
    "display(df_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "af03745f-7433-4f9c-b6be-f9ed428d7f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Derive Flags & Columns (Business Logic)\n",
    "# Source File: json\n",
    "# Scenario: Identify high-value shipments for security tracking.\n",
    "# Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "# Scenario: Flag weekend operations for overtime calculation.\n",
    "# # Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "#dayofweek - return day of the week as int, isin check if the value is in the list, detect weekend in pyspark, 1-sunday, 7-saturday respectively\n",
    "df_dflag = df_route.withColumns(\n",
    "    {\n",
    "    \"is_weekend\": when(\n",
    "        dayofweek(col(\"shipment_date\")).isin([1, 7]),\n",
    "        True\n",
    "    ).otherwise(False),\n",
    "\n",
    "    \"is_high_value\": when(\n",
    "        col(\"shipment_cost\") > 50000,\n",
    "        True\n",
    "    ).otherwise(False)\n",
    "})\n",
    "\n",
    "display(df_dflag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "58f4e06e-db1e-4289-9301-ed6da5e95592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# **4. Format (Standardization)**<br>\n",
    "# Source File: json<br>\n",
    "# * **Scenario:** Finance requires readable currency formats.\n",
    "# * **Action:** Format `shipment_cost` to string like **\"â‚¹30,695.80\"**. \n",
    "df_dformat=df_dflag.withColumn(\"shipment_cost\", concat(lit(\"â‚¹\"),format_number(col(\"shipment_cost\"), 2)))\n",
    "# format_number - Formats the number - format like '99,99,999.00', d decimal places and returns the result as a string.\n",
    "display(df_dformat)\n",
    "\n",
    "# * **Scenario:** Standardize city names for reporting.\n",
    "# * **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" â†’ **\"CHENNAI\"**).\n",
    "df_dformat=df_dformat.withColumn(\"source_city\", upper(col(\"source_city\")))\n",
    "display(df_dformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3e443f5a-6fc9-481e-bbcc-e91144b1a344",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768324590502}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Group & Aggregate (Summarization)\n",
    "# Source Files: logistics_source1 and logistics_source2\n",
    "# Scenario: Regional staffing analysis.\n",
    "# Action: Group by hub_location and Count the number of staff.\n",
    "df_location = df_fullname.groupBy(\"origin_hub_city\").agg(count(\"full_name\").alias(\"staff_count\"))\n",
    "display(df_location)\n",
    "# Scenario: Fleet capacity analysis.\n",
    "# Action: Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "df_vehicle_type = df_route.groupBy(\"vehicle_type\").agg(round(sum(\"shipment_weight_kg\"),2).alias(\"total_weight\"))\n",
    "display(df_vehicle_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69146b8-267d-40a8-9b59-1912a76da848",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768677290049}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 7. Limit (Top-N Analysis)\n",
    "# Source File: json\n",
    "\n",
    "# Scenario: Dashboard snapshot of critical delays.\n",
    "# Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "df_topn = df_route.select(\"shipment_id\",\"order_id\",\"shipment_cost\",\"shipment_status\").filter(col(\"shipment_status\") == \"DELAYED\").orderBy(col(\"shipment_cost\").desc()).limit(10)\n",
    "display(df_topn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba266dd-10a6-4f9c-afb6-4113c04708fe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768327303990}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Sorting (Ordering)\n",
    "# Source File: json\n",
    "# Scenario: Prioritize the most expensive shipments.\n",
    "# Action: Sort by shipment_cost in Descending order.\n",
    "df_json_sort = df_route.orderBy(col(\"shipment_cost\").desc())\n",
    "display(df_json_sort)\n",
    "# Scenario: Organize daily dispatch schedule.\n",
    "# Action: Sort by shipment_date (Ascending) then priority_flag (Descending).\n",
    "df_route = df_route.withColumn(\"priority_flag\",when((col(\"shipment_date\") <= lit(\"2024-04-30\")),True).otherwise(False))\n",
    "df_priority_sort = df_route.orderBy(col(\"shipment_date\").asc(), col(\"priority_flag\").desc())\n",
    "display(df_priority_sort)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4500ea-13e8-423e-9822-78a3110d4e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Joins\n",
    "# Source Files:\n",
    "# Left Side (staff_df):\n",
    "# logistics_source1 & logistics_source2\n",
    "# Right Side (shipments_df):\n",
    "# logistics_shipment_detail_3000.json\n",
    "\n",
    "# 1.1 Frequently Used Simple Joins (Inner, Left)\n",
    "# Inner Join (Performance Analysis):\n",
    "# Scenario: We only want to analyze completed work. Connect Staff to the Shipments they handled.\n",
    "# Action: Join staff_df and shipments_df on shipment_id.\n",
    "# Result: Returns only rows where a staff member is assigned to a valid shipment.\n",
    "\n",
    "df_fullname=dfunion.withColumn(\"shipment_id\", col(\"shipment_id\").try_cast(\"int\"))\n",
    "staff_df=df_fullname\n",
    "shipments_df=df_route\n",
    "df_innerjoin = staff_df.join(shipments_df, on=\"shipment_id\", how=\"inner\").select(\"shipment_id\", \"first_name\", \"role\", \"data_source\", \"order_id\")\n",
    "display(df_innerjoin)\n",
    "\n",
    "# Left Join (Idle Resource check):\n",
    "# Scenario: Find out which staff members are currently idle (not assigned to any shipment).\n",
    "# Action: Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.shipment_id is NULL.\n",
    "\n",
    "df_leftjoin = staff_df.alias(\"staff_df\").join(shipments_df.alias(\"shipments_df\"), on=\"shipment_id\", how=\"left\").filter(col(\"shipments_df.shipment_id\").isNull()).select(\n",
    "    col(\"staff_df.shipment_id\").alias('Csv_shipment_id'), \n",
    "       \"first_name\", \n",
    "       \"role\", \n",
    "       \"data_source\", \n",
    "       \"order_id\", \n",
    "       col(\"shipments_df.shipment_id\").alias('Json_shipment_id'))\n",
    "display(df_leftjoin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a1cf24e-04b7-4c80-97f3-a0c754ca8a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)\n",
    "# Self Join (Peer Finding):\n",
    "# Scenario: Find all pairs of employees working in the same hub_location.\n",
    "# Action: Join staff_df to itself on hub_location, filtering where staff_id_A != staff_id_B.\n",
    "\n",
    "df_selfjoin = staff_df.alias(\"A\").join(staff_df.alias(\"B\"), on=\"hub_location\", how=\"inner\").filter(\n",
    "    col(\"A.shipment_id\") != col(\"B.shipment_id\")).select(\n",
    "    col(\"A.shipment_id\").alias('staff_id_A'), \n",
    "       \"A.first_name\", \n",
    "       \"A.last_name\", \n",
    "       \"A.hub_location\", \n",
    "       col(\"B.shipment_id\").alias('staff_id_B'), \n",
    "       col(\"B.first_name\").alias('first_name_B'), \n",
    "       col(\"B.last_name\").alias('last_name_B'))\n",
    "display(df_selfjoin)\n",
    "\n",
    "# Right Join (Orphan Data Check):\n",
    "# Scenario: Identify shipments in the system that have no valid driver assigned (Data Integrity Issue).\n",
    "# Action: Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side.\n",
    "\n",
    "df_rightjoin = staff_df.alias(\"staff_df\").join(shipments_df.alias(\"shipments_df\"), on=\"shipment_id\", how=\"right\").filter(col(\"staff_df.shipment_id\").isNull()).select(\n",
    "    col(\"staff_df.shipment_id\").alias('Csv_shipment_id'), \n",
    "       \"first_name\", \n",
    "       \"role\", \n",
    "       \"order_id\", \n",
    "       col(\"shipments_df.shipment_id\").alias('Json_shipment_id'),\n",
    "       \"shipment_status\")\n",
    "display(df_rightjoin)\n",
    "\n",
    "# Full Outer Join (Reconciliation):\n",
    "# Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "# Action: Perform a Full Outer Join on shipment_id.\n",
    "\n",
    "df_fullouterjoin = staff_df.alias(\"staff_df\").join(shipments_df.alias(\"shipments_df\"), on=\"shipment_id\", how=\"fullouter\").select(\n",
    "    col(\"staff_df.shipment_id\").alias('Csv_shipment_id'),\n",
    "    \"first_name\", \n",
    "       \"role\", \n",
    "       col(\"shipments_df.shipment_id\").alias('Json_shipment_id'))\n",
    "display(df_fullouterjoin)\n",
    "\n",
    "# Cartesian/Cross Join (Capacity Planning):\n",
    "# Scenario: Generate a schedule of every possible driver assignment to every pending shipment to run an optimization algorithm.\n",
    "# Action: Cross Join drivers_df and pending_shipments_df.\n",
    "display(staff_df)\n",
    "staff_driver_df = staff_df.filter(col(\"role\") == \"Driver\")\n",
    "pending_shipments_df = shipments_df.filter(col(\"shipment_status\").isin(\"CREATED\",\"DELAYED\",\"IN_TRANSIT\"))\n",
    "df_capacity_plan = staff_driver_df.alias(\"Csv_Id\").crossJoin(pending_shipments_df).select(\"role\", col(\"Csv_Id.shipment_id\"), \"shipment_status\")\n",
    "display(df_capacity_plan)\n",
    "display(pending_shipments_df.count())#json - pending status - 1776 rows\n",
    "display(staff_driver_df.count())#csv - driver role count - 18 rows\n",
    "display(df_capacity_plan.count())#cross - capacity plan - 31968 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "77d071e4-1928-46dc-afba-5085585d15c2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768416507063}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1.3 Advanced Joins (Semi and Anti)\n",
    "# Left Semi Join (Existence Check):\n",
    "# Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "# Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "# Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "df_leftsemi = staff_df.join(shipments_df, on=\"shipment_id\", how=\"left_semi\").filter(col(\"role\") == \"Driver\").select(\n",
    "       \"shipment_id\", \n",
    "       \"first_name\", \n",
    "       \"role\")   \n",
    "display(df_leftsemi)# returns only left table cols so no need to select right table cols - right semi anti concept is not available\n",
    "\n",
    "# Left Anti Join (Negation Check):\n",
    "# Scenario: \"Show me the details of Drivers who have never touched a shipment.\"\n",
    "# Action: staff_df.join(shipments_df, \"shipment_id\", \"left_anti\").\n",
    "df_leftanti = staff_df.join(shipments_df, on=\"shipment_id\", how=\"left_anti\").filter(col(\"role\") == \"Driver\").select(\n",
    "       \"shipment_id\", \n",
    "       \"first_name\", \n",
    "       \"role\")   \n",
    "display(df_leftanti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88815e20-5229-4cb6-bdc9-1d2edf563674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Lookup\n",
    "# Source File: logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "\n",
    "# Scenario: Validation. Check if the hub_location in the staff file exists in the corporate Master_City_List.\n",
    "# Action: Compare values against a reference list.\n",
    "\n",
    "geo_data = [\n",
    "    (\"AbuDhabi\", 24.4539, 54.3773),\n",
    "    (\"Amritsar\", 31.6340, 74.8723),\n",
    "    (\"Bangalore\", 12.9716, 77.5946),\n",
    "    (\"Birmingham\", 52.4862, -1.8904),\n",
    "    (\"Boston\", 42.3601, -71.0589),\n",
    "    (\"California\", 36.7783, -119.4179),\n",
    "    (\"Chennai\", 13.0827, 80.2707),\n",
    "    (\"Coimbatore\", 11.0168, 76.9558),\n",
    "    (\"Delhi\", 28.7041, 77.1025),\n",
    "    (\"Dubai\", 25.2048, 55.2708),\n",
    "    (\"HongKong\", 22.3193, 114.1694),\n",
    "    (\"Hyderabad\", 17.3850, 78.4867),\n",
    "    (\"Indore\", 22.7196, 75.8577),\n",
    "    (\"Jaipur\", 26.9124, 75.7873),\n",
    "    (\"Kochi\", 9.9312, 76.2673),\n",
    "    (\"London\", 51.5074, -0.1278),\n",
    "    (\"Lucknow\", 26.8467, 80.9462),\n",
    "    (\"Mumbai\", 19.0760, 72.8777),\n",
    "    (\"NewYork\", 40.7128, -74.0060),\n",
    "    (\"Pune\", 18.5204, 73.8567),\n",
    "    (\"Scranton\", 41.4089, -75.6624),\n",
    "    (\"Singapore\", 1.3521, 103.8198),\n",
    "    (\"Tokyo\", 35.6762, 139.6503)\n",
    "]\n",
    "\n",
    "Master_City_List = spark.createDataFrame(geo_data, [\"master_hub_location\", \"latitude\", \"longitude\"])\n",
    "display(Master_City_List)\n",
    "\n",
    "df_joined = staff_df.join(\n",
    "    Master_City_List,\n",
    "    staff_df.hub_location == Master_City_List.master_hub_location,\n",
    "    \"left\"\n",
    ")\n",
    "df_lookup = (df_joined.withColumn(\n",
    "    \"location_status\",\n",
    "    when(col(\"master_hub_location\").isNotNull(), \"valid\")\n",
    "    .otherwise(\"invalid\")).select(\n",
    "        \"shipment_id\", \"first_name\", \"role\", \"hub_location\", \"location_status\")\n",
    ")\n",
    "display(df_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34513d01-dab1-47bb-b294-ebaa43bdadd6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768481532607}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Lookup & Enrichment\n",
    "# Source File: logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "\n",
    "# Scenario: Geo-Tagging.\n",
    "# Action: Lookup hub_location (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding lat and long columns for map plotting.\n",
    "df_enrichment = (df_joined.\\\n",
    "                withColumn(\"lat\", when(col(\"hub_location\") == \"Pune\", col(\"latitude\")).otherwise(None)).\\\n",
    "                withColumn(\"long\", when(col(\"hub_location\") == \"Pune\", col(\"longitude\")).otherwise(None))).select(\n",
    "                \"shipment_id\", \"first_name\", \"role\", \"hub_location\", \"lat\", \"long\"\n",
    ")\n",
    "display(df_enrichment)\n",
    "\n",
    "# 4. Schema Modeling (Denormalization)\n",
    "# Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)\n",
    "\n",
    "# Scenario: Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "# Action: Flatten the Star Schema. Join Staff, Shipments, and Vehicle_Master into one wide table (wide_shipment_history) so analysts don't have to perform joins during reporting.\n",
    "df_wide_shipment_history = staff_df.join(shipments_df, on = \"shipment_id\", how = \"inner\")\n",
    "display(df_wide_shipment_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f150a38-6e6a-46aa-b160-0b5478e63375",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768486494230}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Windowing (Ranking & Trends)\n",
    "# Source Files:\n",
    "# logistics_source2: Provides hub_location (Partition Key).\n",
    "# logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "# Scenario: \"Who are the Top 3 Drivers by Cost in each Hub?\"\n",
    "# Action:\n",
    "# Partition by hub_location.\n",
    "# Order by total_shipment_cost Descending.\n",
    "# Apply dense_rank() and `row_number()\n",
    "# Filter where rank or row_number <= 3.\n",
    "from pyspark.sql.window import Window\n",
    "dfls2=dfls2.withColumn(\"shipment_id\", col(\"shipment_id\").try_cast(\"int\")).dropna(subset=[\"hub_location\"])\n",
    "\n",
    "# df_window_join = dfls2.join(shipments_df, on=\"shipment_id\", how=\"inner\")\n",
    "df_window_join = (\n",
    "    dfls2.alias(\"csv\").join(shipments_df.alias(\"json\"), on=\"shipment_id\", how=\"inner\")\n",
    "        .select(\n",
    "            col(\"shipment_id\"),\n",
    "            col(\"first_name\"),\n",
    "            col(\"last_name\"),\n",
    "            col(\"age\"),\n",
    "            col(\"role\"),\n",
    "            col(\"hub_location\"),\n",
    "            col(\"csv.vehicle_type\").alias(\"driver_vehicle_type\"),\n",
    "            col(\"json.vehicle_type\").alias(\"shipment_vehicle_type\"),\n",
    "            # col(\"csv.data_source\"), \n",
    "            col(\"order_id\"),\n",
    "            col(\"shipment_status\"),\n",
    "            col(\"cargo_type\"),\n",
    "            col(\"payment_mode\"),\n",
    "            col(\"json.source_city\"),\n",
    "            col(\"json.destination_city\"),\n",
    "            col(\"shipment_weight_kg\"),\n",
    "            col(\"shipment_cost\"),\n",
    "            col(\"shipment_date\"),\n",
    "            col(\"vehicle_id\"),\n",
    "            col(\"ship_year\"),\n",
    "            col(\"ship_month\"),\n",
    "            col(\"is_weekend\")\n",
    "        )\n",
    ")\n",
    "# display(df_window)\n",
    "window_spec = Window.partitionBy(\"hub_location\").orderBy(desc(\"shipment_cost\"))\n",
    "\n",
    "df_partition_window = df_window_join.withColumn(\"seqnum\", row_number().over(window_spec))\n",
    "df_dense = df_partition_window.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "df_ranked = df_dense.filter(col(\"seqnum\") <= 3).select(\"hub_location\",\"dense_rank\", \"seqnum\",\"shipment_cost\")\n",
    "display(df_ranked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909f69b6-4777-402a-823d-e08dd0de9625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Analytical Functions (Lead/Lag)\n",
    "# Source File:\n",
    "# logistics_shipment_detail_3000.json\n",
    "# Scenario: Idle Time Analysis.\n",
    "# Action: For each driver, calculate the days elapsed since their previous shipment.\n",
    "\n",
    "# from pyspark.sql.functions import col, to_date\n",
    "# shipments_df = shipments_df.withColumn(\n",
    "#     \"shipment_date\",\n",
    "#     to_date(col(\"shipment_date\"), \"yyyy-MM-dd\")\n",
    "# )\n",
    "\n",
    "# driver_df = staff_df.filter(\"role == 'Driver'\") #.select(col(\"shipment_id\").alias(\"driver_id\"))\n",
    "# display(driver_df)\n",
    "df_join_driver = staff_df.join(shipments_df, on=\"shipment_id\", how=\"inner\")\n",
    "# display(df_join_driver)\n",
    "\n",
    "window_spec = Window.partitionBy(\"shipment_id\").orderBy(\"shipment_date\")\n",
    "\n",
    "shipments_lag = df_join_driver.withColumn(\n",
    "    \"previous_shipment_date\",\n",
    "    lag(\"shipment_date\").over(window_spec)\n",
    ")\n",
    "\n",
    "idle_time_df = shipments_lag.withColumn(\n",
    "    \"idle_days\",\n",
    "    datediff(col(\"shipment_date\"), col(\"previous_shipment_date\"))\n",
    ").select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\",\n",
    "    \"previous_shipment_date\",\n",
    "    \"idle_days\")\n",
    "display(idle_time_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f31229e-6c61-49a5-b2c4-c4fc4a950834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Set Operations\n",
    "# Source Files: logistics_source1 and logistics_source2\n",
    "\n",
    "# Union: Combining Source1 (Legacy) and Source2 (Modern) into one dataset (Already done in Active Munging).\n",
    "# Intersect: Identifying Staff IDs that appear in both Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "# Except (Difference): Identifying Staff IDs present in Source 2 but missing from Source 1 (New Hires).\n",
    "\n",
    "dfls1 = spark.read.csv(\"/Volumes/workspace/default/logistics_project_data/logistics_source1\",header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record')\n",
    "\n",
    "dfls2=spark.read.csv(path=[\"/Volumes/workspace/default/logistics_project_data/logistics_source2\"],header= True , inferSchema=False , columnNameOfCorruptRecord='corrupt_record')\n",
    "\n",
    "file1 = [\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\"]\n",
    "\n",
    "df1 = dfls1.select(file1)\n",
    "df2 = dfls2.select(file1)\n",
    "\n",
    "dfunion_set=df1.union(df2).distinct()\n",
    "display(dfunion_set)\n",
    "\n",
    "df_intersect = df1.intersect(df2)\n",
    "display(df_intersect)\n",
    "df_except = df2.exceptAll(df1)\n",
    "display(df_except)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae731d7-3b35-4112-98fd-2944e86ae343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Grouping & Aggregations (Advanced)\n",
    "# Source Files:\n",
    "# logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).\n",
    "# logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).\n",
    "\n",
    "# Scenario: The CFO wants a subtotal report at multiple levels:\n",
    "# Total Cost by Hub.\n",
    "# Total Cost by Hub AND Vehicle Type.\n",
    "# Grand Total.\n",
    "# Action: Use cube(\"hub_location\", \"vehicle_type\") or rollup() to generate all these subtotals in a single query.\n",
    "# sr2=dfls2.alias(\"sr2\")\n",
    "# js=dfjson.alias(\"js\")\n",
    "# from pyspark.sql.functions import sum\n",
    "# df_cube = df_window_join.cube(col(\"sr2.hub_location\"), col(\"js.vehicle_type\")).agg(\n",
    "#     sum(col(\"js.shipment_cost\").alias(\"total_cost\")\n",
    "# ))\n",
    "# display(df_cube)\n",
    "\n",
    "# df_cube = (\n",
    "#     df_window_join\n",
    "#     .cube(\"hub_location\", \"vehicle_type\")\n",
    "#     .agg(sum(\"shipment_cost\").alias(\"total_cost\"))\n",
    "# )\n",
    "\n",
    "# display(df_cube)\n",
    "# df_clean = (\n",
    "#     df_window_join\n",
    "#     .withColumnRenamed(\"vehicle_type\", \"vehicle_type_src\")\n",
    "# )\n",
    "\n",
    "# df_cube = (\n",
    "#     df_clean\n",
    "#     .cube(\"hub_location\", \"vehicle_type_src\")\n",
    "#     .agg(sum(\"shipment_cost\").alias(\"total_cost\"))\n",
    "# )\n",
    "\n",
    "# #\n",
    "# df_cube = (\n",
    "#     df_window_join\n",
    "#         .cube(\"hub_location\", \"shipment_vehicle_type\", \"driver_vehicle_type\")\n",
    "#         .agg(sum(\"shipment_cost\").alias(\"total_cost\"))\n",
    "# )\n",
    "# display(df_cube)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d665eeb-2d95-440f-85c1-04e8a30ae311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfls2 = dfls2.select(\"vehicle_type\", \"hub_location\").dropDuplicates()\n",
    "staff = dfls2.alias(\"staff\")\n",
    "# shipments = shipments_df.alias(\"shipments\")\n",
    "# staff = dfls2.withColumn(\"shipment_id\", col(\"shipment_id\").try_cast(\"int\"))\n",
    "df_window_join = staff.join(\n",
    "    dfjson,\n",
    "    on=\"vehicle_type\",\n",
    "    how=\"full_outer\"\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df_cube = df_window_join.cube(\n",
    "    col(\"hub_location\"), \n",
    "    col(\"vehicle_type\")   # explicitly from staff\n",
    ").agg(\n",
    "    sum(\"shipment_cost\").alias(\"total_cost\")  # explicitly from shipments\n",
    ")\n",
    "df_cube = df_cube.orderBy(\"hub_location\", \"vehicle_type\")\n",
    "display(df_cube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4290155146768354,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_DSL_SQL_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
